{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLfRBCjm10f-"
      },
      "source": [
        "# HPC Mini-Challenge 2 - Beschleunigung in Data Science\n",
        "## Teil 2: GPU\n",
        "#### FHNW - FS22\n",
        "\n",
        "Original von S. Suter, angepasst für das HS22 von S. Marcin\n",
        "\n",
        "Abgabe von: <font color='blue'>Joseph Weibel</font>\n",
        "\n",
        "#### Ressourcen\n",
        "* [Überblick GPU Programmierung](https://www.cherryservers.com/blog/introduction-to-gpu-programming-with-cuda-and-python)\n",
        "* [CUDA Basic Parts](https://nyu-cds.github.io/python-gpu/02-cuda/)\n",
        "* [Accelerate Code with CuPy](https://towardsdatascience.com/heres-how-to-use-cupy-to-make-numpy-700x-faster-4b920dda1f56)\n",
        "* Vorlesungen und Beispiele aus dem Informatikkurs PAC (parallel computing), siehe resourcen\n",
        "* CSCS \"High-Performance Computing with Python\" Kurs, Tag 3: \n",
        "    - JIT Numba GPU 1 + 2\n",
        "    - https://youtu.be/E4REVbCVxNQ\n",
        "    - https://github.com/eth-cscs/PythonHPC/tree/master/numba-cuda\n",
        "    - Siehe auch aktuelles Tutorial von 2021\n",
        "* [Google CoLab](https://colab.research.google.com/) oder ggf. eigene GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ny9XUwE10gA",
        "outputId": "b4fe63d9-3416-4eb0-a428-333a4ef97beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (0.56.4)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba) (0.39.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba) (4.13.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.18 in /usr/local/lib/python3.8/dist-packages (from numba) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba) (57.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba) (3.11.0)\n"
          ]
        }
      ],
      "source": [
        "#!pip install numba"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz9j6XIIJoqn",
        "outputId": "a2516608-d300-4648-d2b7-e665e875b0ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 15 09:39:02 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   72C    P0    29W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {
          "is_executing": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfDMy5al10gB",
        "outputId": "7e5e74f9-749d-4015-c4da-e5155315fcf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/numba/cuda/dispatcher.py:488: NumbaPerformanceWarning: Grid size 4 will likely result in GPU under-utilization due to low occupancy.\n",
            "  warn(NumbaPerformanceWarning(msg))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.       ,  1.       ,  1.4142135, ..., 63.97656  , 63.98437  ,\n",
              "       63.992188 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Dummy Beispiel zum testen mit Numba\n",
        "\n",
        "import math\n",
        "from numba import vectorize\n",
        "import numpy as np\n",
        "\n",
        "@vectorize(['float32(float32)'], target='cuda')\n",
        "def gpu_sqrt(x):\n",
        "    return math.sqrt(x)\n",
        "  \n",
        "\n",
        "a = np.arange(4096,dtype=np.float32)\n",
        "gpu_sqrt(a)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBfhyWv1SnQV",
        "outputId": "05f9630f-c231-4eed-87ea-11158928de55"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import imageio\n",
        "import numpy as np\n",
        "\n",
        "subfolder = '001'\n",
        "folders = os.path.join('/content/drive/MyDrive/adni_png', subfolder)\n",
        "\n",
        "images = np.empty([7,256,170])\n",
        "idx = 0\n",
        "names = []\n",
        "for filename in os.listdir(folders):\n",
        "    if filename.endswith('.png') and '145' in filename:\n",
        "        with open(os.path.join(folders, filename), 'r') as f:\n",
        "            im = imageio.imread(f.name)\n",
        "            names.insert(idx,f.name[-17:-4])\n",
        "            images[idx,:,:] = im\n",
        "            print (names[idx], im.shape)\n",
        "            idx += 1\n",
        "            \n",
        "print(images.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "C8bXzEUrSh7-",
        "outputId": "2c6e05c2-78eb-4323-c318-2de64d71cd25"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-28980cef9b40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'145'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/adni_png/001'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m = images[0]\n",
        "m = m -m.min() / m.max() - m.min() # normalize data \n",
        "u,s,vt = np.linalg.svd(m, full_matrices=False)"
      ],
      "metadata": {
        "id": "mS8O4WkCTuRK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "m = np.random.normal(size=(1000, 1000))\n",
        "u, s, vt = np.linalg.svd(m, full_matrices=False)"
      ],
      "metadata": {
        "id": "iW81HIzP5E1n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kg9v73Xr10gC"
      },
      "source": [
        "### 5 GPU Rekonstruktion\n",
        "\n",
        "Implementiere eine SVD-Rekonstruktionsvariante auf der GPU oder in einem hybriden Setting. Code aus dem ersten Teil darf dabei verwendet werden. Wähle  bewusst, welche Teile des Algorithms in einem GPU Kernel implementiert werden und welche effizienter auf der CPU sind. Ziehe dafür Erkenntnisse aus dem ersten Teil mit ein. Es muss mindestens eine Komponente des Algorithmuses in einem GPU-Kernel implementiert werden. Dokumentiere Annahmen, welche du ggf. zur Vereinfachung triffst. Evaluiere, ob du mit CuPy oder Numba arbeiten möchtest. Diskutiere deine Entscheidungen in 150-200 Wörtern. \n",
        "\n",
        "Links:\n",
        "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bQ_svru010gC"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "from numba import cuda, float32\n",
        "import math\n",
        "\n",
        "\n",
        "@cuda.jit\n",
        "def _reconstruct_svd_gpu_1(u, s, vt, reco):\n",
        "  x, y = cuda.grid(2)\n",
        "\n",
        "  if x < u.shape[0] and y < vt.shape[0]:\n",
        "    sum = 0\n",
        "    for k in range(s.shape[0]):\n",
        "      sum += u[x, k] * s[k] * vt[k, y]\n",
        "\n",
        "    reco[x, y] = sum\n",
        "\n",
        "def reconstruct_svd_gpu_1(u, s, vt, k, threads_per_block=32):\n",
        "  reco_h = np.zeros((u.shape[0], vt.shape[1]))\n",
        "\n",
        "  reco_d = cuda.to_device(reco_h)\n",
        "  u_d = cuda.to_device(u[:, 0:k])\n",
        "  s_d = cuda.to_device(s[0:k])\n",
        "  vt_d = cuda.to_device(vt[0:k, :])\n",
        "\n",
        "  n_blocks_x = math.ceil(u.shape[0] / threads_per_block)\n",
        "  n_blocks_y = math.ceil(vt.shape[0] / threads_per_block)\n",
        "\n",
        "  _reconstruct_svd_gpu_1[\n",
        "    (n_blocks_x, n_blocks_y), (threads_per_block, threads_per_block)\n",
        "  ](u_d, s_d, vt_d, reco_d)\n",
        "\n",
        "  return reco_d.copy_to_host()\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "reco = reconstruct_svd_gpu_1(u, s, vt, u.shape[1])\n",
        "np.testing.assert_array_almost_equal(reco, m, decimal=3)"
      ],
      "metadata": {
        "id": "7Yp2DK46SbZL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7r60ZT_10gD"
      },
      "source": [
        "<font color='blue'>Bei dieser einfachen Implementierung wird die gesamte Berechnung auf der GPU vorgenommen. Jeder Thread ist für ein einzelnes Pixel in der Rekonstruktion verantwortlich. Das Kürzen der Matrizen und Vektoren auf n Komponenten wird auf der CPU vorgenommen, beziehungsweise werden nur die nötigen Elemente auf die GPU kopiert. Dadurch wird verhindert, dass Elemente zur GPU kopiert werden, die dort gar nicht benötigt werden.\n",
        "\n",
        "Die Anzahl Threads pro Block kann dabei frei definiert werden. Kleinere Grössen können sinnvoll sein, um dadurch die Rekonstruktionsgrösse genau aufteilen zu können, so dass möglichst keine Threads leer laufen. Bei einer Breite von 2000 px wäre es sinnvoller 25 oder 16 statt 32 zu verwenden.\n",
        "TODO: stimmt das?\n",
        "\n",
        "Es wird numba verwendet, da damit die grundelegenden GPU-Operationen simuliert werden können. Zudem kann damit Python-Code geschrieben werden, der dann autoamtisch in C-Code übersetzt wird.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQz_TGwG10gD"
      },
      "source": [
        "#### 5.2 GPU-Kernel Performance\n",
        "\n",
        "##### 5.2.1 Blocks und Strided Access\n",
        "\n",
        "Links: \n",
        "* [Examples: Matrix Multiplikation](https://numba.readthedocs.io/en/latest/cuda/examples.html)\n",
        "* [NVIDIA Kapitel zu \"Strided Access\"](https://spaces.technik.fhnw.ch/multimediathek/file/cuda-best-practices-in-c)\n",
        "* https://developer.nvidia.com/blog/cublas-strided-batched-matrix-multiply/\n",
        "* https://developer.nvidia.com/blog/how-access-global-memory-efficiently-cuda-c-kernels/\n",
        "\n",
        "Führe 2-3 Experimente mit unterschiedlichen Blockkonfigurationen und Strided Access durch, auf welchen dein GPU-Kernel arbeitet. Messe die Performance des GPU-Kernels mittels geeigneten Funktionen. Welche Blockgrösse und welcher Strided Access hat sich bei dir basierend auf deinen Experimenten als am erfolgreichsten erwiesen? Welches sind deiner Meinung nach die Gründe dafür? Wie sind die Performance Unterschiede zwischen deiner CPU und PGU Implementierung? Diskutiere deine Analyse in ca. 200 Wörtern und ggf. mit Grafiken. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# os.environ[\"NUMBA_ENABLE_CUDASIM\"] = \"1\"\n"
      ],
      "metadata": {
        "id": "fi3hKRXbZXZ9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3aYPqmRk10gE"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "@cuda.jit\n",
        "def _reconstruct_svd_gpu_2(u, s, vt, reco):\n",
        "  # each thread calculates the sum of products for a specific index (x, y)\n",
        "\n",
        "  x, y = cuda.grid(2)\n",
        "  local_x = cuda.threadIdx.x\n",
        "  local_y = cuda.threadIdx.y\n",
        "  threads_per_block = 32 # cuda.blockDim.x but must be constant\n",
        "  blocks_per_grid = cuda.gridDim.x\n",
        "\n",
        "  shrd_u = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_s = cuda.shared.array(shape=(threads_per_block,), dtype=float32)\n",
        "  shrd_vt = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "\n",
        "  sum_of_products = float32(0.)\n",
        "  for block in range(blocks_per_grid):\n",
        "    # calculate sum of products per block\n",
        "    # the block is only moved to get the other values in the matrices u, s and vt\n",
        "    # the index for which the sum of product is calculated remains the same\n",
        "\n",
        "    shrd_u[local_y, local_x] = 0\n",
        "    if y < u.shape[0] and (block * threads_per_block + local_x) < u.shape[1]:\n",
        "      shrd_u[local_y, local_x] = u[y, block * threads_per_block + local_x]\n",
        "\n",
        "    if local_y == 0:\n",
        "      shrd_s[local_x] = 0\n",
        "      if (block * threads_per_block + local_x) < s.shape[0]:\n",
        "        # only first row in block loads shrd_s since it's a one-dimensional array\n",
        "        shrd_s[local_x] = s[block * threads_per_block + local_x]\n",
        "\n",
        "    shrd_vt[local_y, local_x] = 0\n",
        "    if x < vt.shape[1] and (block * threads_per_block + local_y) < vt.shape[0]:\n",
        "      shrd_vt[local_y, local_x] = vt[block * threads_per_block + local_y, x]\n",
        "\n",
        "    # wait until all tpb x tpb elements are filled\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # start calculating the sum of products for index (y, x)\n",
        "    for i in range(threads_per_block):\n",
        "      # no checking of boundaries necessary since the warp executes the\n",
        "      # statement anyway and threads out of bound would have to wait anyway\n",
        "      sum_of_products += shrd_u[local_y, i] * shrd_s[i] * shrd_vt[i, local_x]\n",
        "\n",
        "    # wait until all threads have computed their sum of products before we move\n",
        "    # to the next block as the shared values will be overridden with the next\n",
        "    # iteration\n",
        "    cuda.syncthreads()\n",
        "\n",
        "  if y < reco.shape[0] and x < reco.shape[1]:\n",
        "    reco[y, x] = sum_of_products\n",
        "\n",
        "def reconstruct_svd_gpu_2(u, s, vt, k):\n",
        "  threads_per_block = 32\n",
        "  reco_h = np.zeros((u.shape[0], vt.shape[1]))\n",
        "\n",
        "  reco_d = cuda.to_device(reco_h)\n",
        "  u_d = cuda.to_device(u[:, 0:k])\n",
        "  s_d = cuda.to_device(s[0:k])\n",
        "  vt_d = cuda.to_device(vt[0:k, :])\n",
        "\n",
        "  grid_y_max = max(u.shape[0], k)\n",
        "  grid_x_max = max(k, vt.shape[1])\n",
        "\n",
        "  n_blocks_x = math.ceil(grid_x_max / threads_per_block)\n",
        "  n_blocks_y = math.ceil(grid_y_max / threads_per_block)\n",
        "\n",
        "  _reconstruct_svd_gpu_2[\n",
        "    (n_blocks_x, n_blocks_y), (threads_per_block, threads_per_block)\n",
        "  ](u_d, s_d, vt_d, reco_d)\n",
        "\n",
        "  return reco_d.copy_to_host()\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reco = reconstruct_svd_gpu_2(u, s, vt, u.shape[1])\n",
        "np.testing.assert_array_almost_equal(reco, m, decimal=3)\n"
      ],
      "metadata": {
        "id": "rxzNDrzE6SwG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nQJeUtV610gF"
      },
      "source": [
        "<font color='blue'>Die erste Version hat ein schlechtes Verhältnis von Memory- zu Computation-Operationen. Es wird bei jeder Berechnung dreimal aufs Memory zugegriffen, ohne dass diese Werte nochmals verwendet werden. Diese optimierte Variante kopiert nun blockweise die benötigten Werte ins Shared-Memory wo sie von allen Threads desselben Blocks ebenfalls verwendet werden können. Da bei diesen Matrixmultiplikationen sehr viele Elemente wiederverwendet werden können, ist diese Implementierung um einiges schneller als die letzte, auch wenn nach wie vor alle Elemente mehrmals kopiert werden, da sie auch von anderen Blocks verwendet werden.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJZydur_10gF"
      },
      "source": [
        "##### 5.2.2 Memoryallokation auf der GPU\n",
        "Führe 2-3 Experimente durch in welchem du unterschiedliche Varianten des Transfers der Daten bzw. der Memory-Allokation auf die GPU miteinander vergleichst. Messe die Varianten mittels geeigneten Methoden. Als Beispiel können hier z.B. unterschiedlich grosse Packages an Daten auf die GPU kopiert und dann verarbeitet werden oder die Daten werden in unterschiedliche Memory-Typen geladen.\n",
        "\n",
        "Links:\n",
        "* [Best Practices Memory Optimizations](https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations)\n",
        "* [Examples: Matrix Multiplikation und Shared Memory](https://numba.readthedocs.io/en/latest/cuda/examples.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Antwort\n",
        "\n",
        "<font color='blue'></font>"
      ],
      "metadata": {
        "id": "XdDmxc38f0Lg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "tZCkT52c10gF"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "@cuda.jit\n",
        "def _reconstruct_svd_gpu_3(u, s, vt, reco):\n",
        "  # each thread calculates the sum of products for a specific index (x, y)\n",
        "\n",
        "  x, y = cuda.grid(2)\n",
        "  local_x = cuda.threadIdx.x\n",
        "  local_y = cuda.threadIdx.y\n",
        "  threads_per_block = 32 # cuda.blockDim.x but must be constant\n",
        "  blocks_per_grid = cuda.gridDim.x\n",
        "\n",
        "  shrd_u = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_s = cuda.shared.array(shape=(threads_per_block,), dtype=float32)\n",
        "  shrd_vt = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "\n",
        "  sum_of_products = float32(0.)\n",
        "  for block in range(blocks_per_grid):\n",
        "    # calculate sum of products per block\n",
        "    # the block is only moved to get the other values in the matrices u, s and vt\n",
        "    # the index for which the sum of product is calculated remains the same\n",
        "\n",
        "    shrd_u[local_y, local_x] = 0\n",
        "    if y < u.shape[0] and (block * threads_per_block + local_x) < u.shape[1]:\n",
        "      # copy values in transposed order\n",
        "      shrd_u[local_y, local_x] = u[y, block * threads_per_block + local_x]\n",
        "\n",
        "    if local_y == 0:\n",
        "      shrd_s[local_x] = 0\n",
        "      if (block * threads_per_block + local_x) < s.shape[0]:\n",
        "        # only first row in block loads shrd_s since it's a one-dimensional array\n",
        "        shrd_s[local_x] = s[block * threads_per_block + local_x]\n",
        "\n",
        "    shrd_vt[local_x, local_y] = 0\n",
        "    if x < vt.shape[1] and (block * threads_per_block + local_y) < vt.shape[0]:\n",
        "      shrd_vt[local_x, local_y] = vt[block * threads_per_block + local_y, x]\n",
        "\n",
        "    # wait until all tpb x tpb elements are filled\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    # start calculating the sum of products for index (y, x)\n",
        "    for i in range(threads_per_block):\n",
        "      # no checking of boundaries necessary since the warp executes the\n",
        "      # statement anyway and threads out of bound would have to wait anyway\n",
        "\n",
        "      # read values from shred_u in transposed order\n",
        "      sum_of_products += shrd_u[local_y, i] * shrd_s[i] * shrd_vt[local_x, i]\n",
        "\n",
        "    # wait until all threads have computed their sum of products before we move\n",
        "    # to the next block as the shared values will be overridden with the next\n",
        "    # iteration\n",
        "    cuda.syncthreads()\n",
        "\n",
        "  if y < reco.shape[0] and x < reco.shape[1]:\n",
        "    reco[y, x] = sum_of_products\n",
        "\n",
        "def reconstruct_svd_gpu_3(u, s, vt, k):\n",
        "  threads_per_block = 32\n",
        "  reco_h = np.zeros((u.shape[0], vt.shape[1]))\n",
        "\n",
        "  reco_d = cuda.to_device(reco_h)\n",
        "  u_d = cuda.to_device(u[:, 0:k])\n",
        "  s_d = cuda.to_device(s[0:k])\n",
        "  vt_d = cuda.to_device(vt[0:k, :])\n",
        "\n",
        "  grid_y_max = max(u.shape[0], k)\n",
        "  grid_x_max = max(k, vt.shape[1])\n",
        "\n",
        "  n_blocks_x = math.ceil(grid_x_max / threads_per_block)\n",
        "  n_blocks_y = math.ceil(grid_y_max / threads_per_block)\n",
        "\n",
        "  _reconstruct_svd_gpu_3[\n",
        "    (n_blocks_x, n_blocks_y), (threads_per_block, threads_per_block)\n",
        "  ](u_d, s_d, vt_d, reco_d)\n",
        "\n",
        "  return reco_d.copy_to_host()\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reco = reconstruct_svd_gpu_3(u, s, vt, u.shape[1])\n",
        "np.testing.assert_array_almost_equal(reco, m, decimal=3)\n"
      ],
      "metadata": {
        "id": "nz_P46K2frSw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jW5QvzaY10gG"
      },
      "source": [
        "Was sind deine Erkenntnisse bzgl. GPU-Memory-Allokation und des Daten-Transferes auf due GPU? Wie hast du ggf. deine Implementierung aus 5.1 angepasst? Diskutiere deine Antwort in ca. 150-200 Wörtern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQIv_HuO10gG"
      },
      "source": [
        "<font color='blue'>Diese Implementierung versucht Bank-Conflicts beim Zugriff auf das Shared-Memory zu vermeiden. Auf `shrt_vt` wurde bisher in einem Warp jeweils pro Iteration auf dieselbe Spalte zugegriffen. Dadurch kam es zu 32 Bank Conflicts, was dazu führte, dass jedes Element separat aus dem Shared-Memory abgerufen werden musste. Neu ist `shrt_vt` transponiert, so dass im Warp jeweils eine ganze Zeile abgerufen werden kann.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mcY4-4w10gG"
      },
      "source": [
        "##### 5.2.3 Bonus: Memoryoptimierung\n",
        "Optimiere die Memory-Allokation in deiner Implementierung, so dass du einen Leistungssteigerung zu einer anderen Variante demonstrieren kannst."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Antwort\n"
      ],
      "metadata": {
        "id": "DzhQFio_0sn0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "jR7tKA0-10gH"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "@cuda.jit\n",
        "def _reconstruct_svd_gpu_4(u, s, vt, reco):\n",
        "  # each thread calculates the sum of products for a specific index (x, y)\n",
        "\n",
        "  x, y = cuda.grid(2)\n",
        "  local_x = cuda.threadIdx.x\n",
        "  local_y = cuda.threadIdx.y\n",
        "  threads_per_block = 32 # cuda.blockDim.x but must be constant\n",
        "  blocks_per_grid = cuda.gridDim.x\n",
        "\n",
        "  shrd_u = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_s = cuda.shared.array(shape=(threads_per_block,), dtype=float32)\n",
        "  shrd_vt = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_u_pre = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_s_pre = cuda.shared.array(shape=(threads_per_block,), dtype=float32)\n",
        "  shrd_vt_pre = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "\n",
        "  shrd_u_pre[local_y, local_x] = 0\n",
        "  if y < u.shape[0] and local_x < u.shape[1]:\n",
        "    # copy values in transposed order\n",
        "    shrd_u_pre[local_y, local_x] = u[y, local_x]\n",
        "\n",
        "  if local_y == 0:\n",
        "    shrd_s_pre[local_x] = 0\n",
        "    if local_x < s.shape[0]:\n",
        "      # only first row in block loads shrd_s since it's a one-dimensional array\n",
        "      shrd_s_pre[local_x] = s[local_x]\n",
        "\n",
        "  shrd_vt_pre[local_x, local_y] = 0\n",
        "  if x < vt.shape[1] and local_y < vt.shape[0]:\n",
        "    shrd_vt_pre[local_x, local_y] = vt[local_y, x]\n",
        "\n",
        "  # wait until all tpb x tpb elements are filled\n",
        "  cuda.syncthreads()\n",
        "\n",
        "  sum_of_products = float32(0.)\n",
        "  for block in range(blocks_per_grid):\n",
        "    # calculate sum of products per block\n",
        "    # the block is only moved to get the other values in the matrices u, s and vt\n",
        "    # the index for which the sum of product is calculated remains the same\n",
        "\n",
        "    # swap shared memory blocks\n",
        "    shrd_u, shrd_u_pre = shrd_u_pre, shrd_u\n",
        "    shrd_s, shrd_s_pre = shrd_s_pre, shrd_s\n",
        "    shrd_vt, shrd_vt_pre = shrd_vt_pre, shrd_vt\n",
        "\n",
        "    next_block = block + 1\n",
        "\n",
        "    # load next block\n",
        "    shrd_u_pre[local_y, local_x] = 0\n",
        "    if y < u.shape[0] and (next_block * threads_per_block + local_x) < u.shape[1]:\n",
        "      # copy values in transposed order\n",
        "      shrd_u_pre[local_y, local_x] = u[y, next_block * threads_per_block + local_x]\n",
        "\n",
        "    if local_y == 0:\n",
        "      shrd_s_pre[local_x] = 0\n",
        "      if (next_block * threads_per_block + local_x) < s.shape[0]:\n",
        "        # only first row in block loads shrd_s since it's a one-dimensional array\n",
        "        shrd_s_pre[local_x] = s[next_block * threads_per_block + local_x]\n",
        "\n",
        "    shrd_vt_pre[local_x, local_y] = 0\n",
        "    if x < vt.shape[1] and (next_block * threads_per_block + local_y) < vt.shape[0]:\n",
        "      shrd_vt_pre[local_x, local_y] = vt[next_block * threads_per_block + local_y, x]\n",
        "\n",
        "    # start calculating the sum of products for index (y, x) and the current block\n",
        "    for i in range(threads_per_block):\n",
        "      # no checking of boundaries necessary since the warp executes the\n",
        "      # statement anyway and threads out of bound would have to wait anyway\n",
        "\n",
        "      # read values from shred_u in transposed order\n",
        "      sum_of_products += shrd_u[local_y, i] * shrd_s[i] * shrd_vt[local_x, i]\n",
        "\n",
        "    # wait until all threads have computed their sum of products before we move\n",
        "    # to the next block as the shared values will be overridden with the next\n",
        "    # iteration\n",
        "    cuda.syncthreads()\n",
        "\n",
        "  if y < reco.shape[0] and x < reco.shape[1]:\n",
        "    reco[y, x] = sum_of_products\n",
        "\n",
        "def reconstruct_svd_gpu_4(u, s, vt, k):\n",
        "  threads_per_block = 32\n",
        "  reco_h = np.zeros((u.shape[0], vt.shape[1]))\n",
        "\n",
        "  reco_d = cuda.to_device(reco_h)\n",
        "  u_d = cuda.to_device(u[:, 0:k])\n",
        "  s_d = cuda.to_device(s[0:k])\n",
        "  vt_d = cuda.to_device(vt[0:k, :])\n",
        "\n",
        "  grid_y_max = max(u.shape[0], k)\n",
        "  grid_x_max = max(k, vt.shape[1])\n",
        "\n",
        "  n_blocks_x = math.ceil(grid_x_max / threads_per_block)\n",
        "  n_blocks_y = math.ceil(grid_y_max / threads_per_block)\n",
        "\n",
        "  _reconstruct_svd_gpu_4[\n",
        "    (n_blocks_x, n_blocks_y), (threads_per_block, threads_per_block)\n",
        "  ](u_d, s_d, vt_d, reco_d)\n",
        "\n",
        "  return reco_d.copy_to_host()\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reco = reconstruct_svd_gpu_4(u, s, vt, u.shape[1])\n",
        "np.testing.assert_array_almost_equal(reco, m, decimal=3)\n"
      ],
      "metadata": {
        "id": "qPUsmKNS6oH-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>Bei dieser Optimierung passiert der Kopiervorgang vom Shared-Memory ins globale Memory nun asynchron zu den Berechnungen. Währenddem das Resultat für den aktuellen Block berechnet wird, werden bereits die Elemente des nächsten Blocks kopiert, so dass sie teilweise oder gar vollständig bereitstehen, wenn die Berechnungen des nächsten Blocks anstehen. Dazu wird jedoch doppelt soviel Speicher im Shared-Memory benötigt. Die GPU kann so aber besser ausgelastet werden.</font>"
      ],
      "metadata": {
        "id": "3RL_2wM05HyR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "@cuda.jit\n",
        "def _reconstruct_svd_gpu_5(u, s, vt, reco):\n",
        "  # each thread calculates the sum of products for a specific index (x, y)\n",
        "\n",
        "  x, y = cuda.grid(2)\n",
        "  local_x = cuda.threadIdx.x\n",
        "  local_y = cuda.threadIdx.y\n",
        "  threads_per_block = 32 # cuda.blockDim.x but must be constant\n",
        "  blocks_per_grid = cuda.gridDim.x\n",
        "\n",
        "  shrd_u = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_s = cuda.shared.array(shape=(threads_per_block,), dtype=float32)\n",
        "  shrd_vt = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_u_pre = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_s_pre = cuda.shared.array(shape=(threads_per_block,), dtype=float32)\n",
        "  shrd_vt_pre = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "\n",
        "  shrd_u_pre[local_y, local_x] = 0\n",
        "  if y < u.shape[0] and local_x < u.shape[1]:\n",
        "    # copy values in transposed order\n",
        "    shrd_u_pre[local_y, local_x] = u[y, local_x]\n",
        "\n",
        "  if local_y == 0:\n",
        "    shrd_s_pre[local_x] = 0\n",
        "    if local_x < s.shape[0]:\n",
        "      # only first row in block loads shrd_s since it's a one-dimensional array\n",
        "      shrd_s_pre[local_x] = s[local_x]\n",
        "\n",
        "  shrd_vt_pre[local_y, local_x] = 0\n",
        "  if x < vt.shape[1] and local_y < vt.shape[0]:\n",
        "    shrd_vt_pre[local_y, local_x] = vt[local_y, x]\n",
        "\n",
        "  # wait until all tpb x tpb elements are filled\n",
        "  cuda.syncthreads()\n",
        "\n",
        "  sum_of_products = float32(0.)\n",
        "  for block in range(blocks_per_grid):\n",
        "    # calculate sum of products per block\n",
        "    # the block is only moved to get the other values in the matrices u, s and vt\n",
        "    # the index for which the sum of product is calculated remains the same\n",
        "\n",
        "    # swap shared memory blocks\n",
        "    shrd_u, shrd_u_pre = shrd_u_pre, shrd_u\n",
        "    shrd_s, shrd_s_pre = shrd_s_pre, shrd_s\n",
        "    shrd_vt, shrd_vt_pre = shrd_vt_pre, shrd_vt\n",
        "\n",
        "    next_block = block + 1\n",
        "\n",
        "    # load next block\n",
        "    shrd_u_pre[local_y, local_x] = 0\n",
        "    if y < u.shape[0] and (next_block * threads_per_block + local_x) < u.shape[1]:\n",
        "      # copy values in transposed order\n",
        "      shrd_u_pre[local_y, local_x] = u[y, next_block * threads_per_block + local_x]\n",
        "\n",
        "    if local_y == 0:\n",
        "      shrd_s_pre[local_x] = 0\n",
        "      if (next_block * threads_per_block + local_x) < s.shape[0]:\n",
        "        # only first row in block loads shrd_s since it's a one-dimensional array\n",
        "        shrd_s_pre[local_x] = s[next_block * threads_per_block + local_x]\n",
        "\n",
        "    shrd_vt_pre[local_y, local_x] = 0\n",
        "    if x < vt.shape[1] and (next_block * threads_per_block + local_y) < vt.shape[0]:\n",
        "      shrd_vt_pre[local_y, local_x] = vt[next_block * threads_per_block + local_y, x]\n",
        "\n",
        "    # start calculating the sum of products for index (y, x) and the current block\n",
        "    for i in range(threads_per_block):\n",
        "      # no checking of boundaries necessary since the warp executes the\n",
        "      # statement anyway and threads out of bound would have to wait anyway\n",
        "\n",
        "      # read values from shred_u in transposed order\n",
        "      sum_of_products += shrd_u[local_y, i] * shrd_s[i] * shrd_vt[i, local_x]\n",
        "\n",
        "    # wait until all threads have computed their sum of products before we move\n",
        "    # to the next block as the shared values will be overridden with the next\n",
        "    # iteration\n",
        "    cuda.syncthreads()\n",
        "\n",
        "  if y < reco.shape[0] and x < reco.shape[1]:\n",
        "    reco[y, x] = sum_of_products\n",
        "\n",
        "def reconstruct_svd_gpu_5(u, s, vt, k):\n",
        "  threads_per_block = 32\n",
        "  reco_h = np.zeros((u.shape[0], vt.shape[1]))\n",
        "\n",
        "  reco_d = cuda.to_device(reco_h)\n",
        "  u_d = cuda.to_device(u[:, 0:k])\n",
        "  s_d = cuda.to_device(s[0:k])\n",
        "  vt_d = cuda.to_device(vt[0:k, :])\n",
        "\n",
        "  grid_y_max = max(u.shape[0], k)\n",
        "  grid_x_max = max(k, vt.shape[1])\n",
        "\n",
        "  n_blocks_x = math.ceil(grid_x_max / threads_per_block)\n",
        "  n_blocks_y = math.ceil(grid_y_max / threads_per_block)\n",
        "\n",
        "  _reconstruct_svd_gpu_5[\n",
        "    (n_blocks_x, n_blocks_y), (threads_per_block, threads_per_block)\n",
        "  ](u_d, s_d, vt_d, reco_d)\n",
        "\n",
        "  return reco_d.copy_to_host()\n",
        "\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "E4CErjPPocD5"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reco = reconstruct_svd_gpu_5(u, s, vt, u.shape[1])\n",
        "np.testing.assert_array_almost_equal(reco, m, decimal=3)\n"
      ],
      "metadata": {
        "id": "1jaqQn-eo2Eg"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>Diese letzte Variante ist nahezu identisch mit der vorhergehenden und setzt auch auf das Preloading vom globalen ins shared Memory. Jedoch werden die Werte aus der Matrix $V^T$ in der ursprünglichen Reihenfolge ins Shared-Memory übernommen und nicht transponiert, um beim Vergleich der Laufzeiten die Optimierung des Vorladens besser quantifizieren zu können.</font>"
      ],
      "metadata": {
        "id": "jDDCKY56sh7F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "import pandas as pd\n",
        "import timeit\n",
        "\n",
        "def plot_runtimes(names, fncs):\n",
        "    times = []\n",
        "    for fnc in fncs:\n",
        "        times.append(timeit.repeat(lambda: fnc(u, s, vt, u.shape[0]), number=3, repeat=30))\n",
        "        \n",
        "    times = np.array(times)\n",
        "    \n",
        "    y = times.mean(axis=1)\n",
        "    plt.figure(figsize = (12, 7))\n",
        "    plt.errorbar(\n",
        "        names,\n",
        "        y,\n",
        "        times.std(axis=1),\n",
        "        linestyle='None',\n",
        "        marker='o', \n",
        "        capsize=10\n",
        "    )\n",
        "    plt.title('Laufzeiten')\n",
        "    plt.xlabel('Variante')\n",
        "    plt.ylabel('Dauer [s] (log)')\n",
        "    plt.yscale('log')\n",
        "    plt.show()\n",
        "\n",
        "    results = pd.DataFrame({'fnc': names, 'mean duration (s)': y})\n",
        "    results.fnc = results.fnc.str.replace('\\n', '')\n",
        "    results\n",
        "\n",
        "plot_runtimes(\n",
        "    ['basic', 'memory optimised', 'avoiding bank conflicts', 'preloading shared memory\\n and avoiding bank conflicts', 'preloading shared memory'],\n",
        "    [reconstruct_svd_gpu_1, reconstruct_svd_gpu_2, reconstruct_svd_gpu_3, reconstruct_svd_gpu_4, reconstruct_svd_gpu_5]\n",
        ")\n",
        "\n",
        "\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "4PWZ5f6U6k7L",
        "outputId": "601332c7-2b23-4c87-b9cf-a66c6dcb462b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color='blue'>Beim Vergleich der Laufzeiten dieser vier GPU-Varianten lässt sich schnell erkennen, dass die Basic-Variante die Vorteile einer GPU nicht nutzt, da sie das Paradigma Single-Instruction-Multiple-Data nicht optimal anwendet. Für jede einzelne Berechnung müssen drei Werte aus dem Global-Memory der GPU kopiert werden, was viel Zeit kostet.\n",
        "\n",
        "Die optimierte Variante kopiert blockweise Werte der zwei Matrizen und des Vektors in das Shared-Memory, wo die Werte vom gesamten Threadblock verwendet werden können. Dadurch wird die Anzahl Zugriffe aufs Global-Memory stark reduziert, was sich einer viel kürzeren Laufzeit bemerkbar macht. Die Vermeidung von Bank-Conflicts bei der dritten Version scheint nicht wie erhofft zu funktionieren. Die Laufzeit erhöht sich bei dieser Version wieder stark, was auch bei der vierten Variante erkennbar ist. Das Preloading vom Global-Memory ins Shared-Memory scheint auch nicht ganz wie erhofft zu funktioniert. Die Laufzeit ist bei dieser Version auch höher als bei Variante 2. Möglicherweise können durch den doppelt so hohen Shared-Memoryverbrauch nicht mehr gleich viele Threads in einem Threadblock gleichzeitig ausgeführt werden, da für die Verwaltung der Threads nicht ausreichend Memory zur Verfügung steht. Die reduzierte Gleichläufigkeit hat dabei einen viel grösseren negativen Effekt als das Vorladen des Memorys wieder wett machen kann.</font>"
      ],
      "metadata": {
        "id": "XqCPW0p2_qcR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-a9oB8m10gH"
      },
      "source": [
        "#### 5.3 NVIDIA Profiler\n",
        "\n",
        "Benutze einen Performance Profiler von NVIDIA, um Bottlenecks in deinem Code zu identifizieren bzw. unterschiedliche Implementierungen (Blocks, Memory etc.) zu vergleichen. \n",
        "\n",
        "* Siehe Beispiel example_profiling_CUDA.ipynb\n",
        "* [Nsight](https://developer.nvidia.com/nsight-visual-studio-edition) für das Profiling des Codes und die Inspektion der Ergebnisse (neuste Variante)\n",
        "* [nvprof](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#nvprof-overview)\n",
        "* [Nvidia Visual Profiler](https://docs.nvidia.com/cuda/profiler-users-guide/index.html#visual)\n",
        "\n",
        "> Du kannst NVIDIA Nsights Systems und den Nvidia Visual Profiler auf deinem PC installieren und die Leistungsergebnisse aus einer Remote-Instanz visualisieren, auch wenn du keine GPU an/in deinem PC hast. Dafür kannst du die ``*.qdrep`` Datei generieren und danach lokal laden.\n",
        "\n",
        "\n",
        "Dokumentiere deine Analyse ggf. mit 1-2 Visualisierungen und beschreibe in 3-5 Sätzen, welche Bottlenecks du gefunden bzw. entschärft hast."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font color=\"blue\">Alle Profiling-Reports wurden mit folgendem Befehl generiert. Das Python-Script berechnet mit SVD die drei Komponenten $U$, $S$ und $V^T$ für eine 2000 x 2000 Elemente grosse Matrix und ruft anschliessend die jeweilige Rekonstruktions-Funktion mit diesen drei Werten auf. Dabei werden alle Komponenten zur Rekonstruktion verwendet.</font>"
      ],
      "metadata": {
        "id": "3IUVged-L4I7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !ncu -f -o reconstruct_svd_gpu_6 --set full --target-processes all python /content/drive/MyDrive/reconstruct_svd_gpu_6.py"
      ],
      "metadata": {
        "id": "X0MM9kUPrVJ4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Variante 2\n",
        "\n",
        "<img src=\"images/reconstruct_svd_gpu_2.png\" alt=\"Report für Variante 2\" width=\"500\"/>\n",
        "\n",
        "<font color=\"blue\">Dass mit dieser Variante eine gute Laufzeit erzielt werden kann, konnten wir bereits feststellen. Auch im Report lässt sich dies erkennen. Einerseits zeigt *NVIDIA Nsight Compute* keine Warnungen an, andererseits sehen wir dass auch an den hohen für die GPU Utilization. Streaming Multiprocessors und das Memory werden zu über 75 % ausgelastet. Wir bewegen uns zudem bei fast 9 FLOPs pro Byte. Es werden also relativ viele Operation pro Memoryzugriff durchgeführt.\n",
        "\n",
        "Im Abschnitt zum Memory-Workload, sehen wir, dass sehr viele Write- und noch mehr Read-Requests ins Shared-Memory abgesetzt werden. Das wird durch die vielen Schreib- und Lesezugriffe für die zwei Matrizen und den einen Vektor verursacht, die wir zur Optimierung der Geschwindigkeit ins Shared-Memory kopieren. Beim Shared-Memory sehen wir zudem, dass keinerleit Bank-Conflicts entstanden sind. Die am meisten beanspruchste Komponente, die LSU-Pipeline, die für die Memory-Zugriffe verantwortlich ist, ist zu 76 % ausgelastet. Es werden also nach wie vor viele Memory-Zugriffe gemacht.</font>"
      ],
      "metadata": {
        "id": "OLJoPDj-LTFn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Variante 3\n",
        "\n",
        "<img src=\"images/reconstruct_svd_gpu_3.png\" alt=\"Report für Variante 3\" width=\"500\"/>\n",
        "\n",
        "<font color=\"blue\">Diese Variante, die eigentlich die Bank-Conflicts reduzieren sollte, führte nun dazu, dass nun sehr viele solche entstehen, wie man bei der Memory-Auswertung sehen kann. Anscheinend war die Verbesserung gar nicht nötig. Aufgrund der vielen Bank Conflicts tauchen nun auch einige Warnungen am Ende des Reports auf. Hier wird der uncoalesced Shared-Memoryzugriff bemängelt. Die Auslastung von *SOL L1/TEX Cache* ist nahezu bei 100 %, was das Bottleneck bei dieser Variante zu sein scheint.\n",
        "\n",
        "Dass die vielen Bank Conflicts die Laufzeit stark verlängern, sieht man oben an der Duration und der Elapsed Cycles, die nun viel höher sind als noch beim letzten Report. Die Wavefronts, also die tatsächlichen separaten Zugriffe aufs Shared-Memory, haben sich fast verzwölffacht (von 768'144'384 auf 8'705'636'352). Ausserdem ist deshalb der Streaming Multiprocessor viel weniger stark ausgelastet (weniger als 10 %). Deshalb befinden sich auch die Warps allermeistens im Zustand *Stall MIO Throttle*, also sie warten bis die Memory-Komponente wieder verfügbar ist, um weitere Memory-Zugriffe auszuführen.</font>\n",
        "\n",
        "##### Allgemein\n",
        "\n",
        "Bei allen Varianten können aufgrund der wenigen If-Abfragen im Code hohe Werte bei der *Branch Efficency* erzielt werden. Das bedeutet, dass wenige Threads auf andere Threads müssen, weil eine If-Abfrage für sie negativ ausfällt. \n"
      ],
      "metadata": {
        "id": "nVdfuhXCQQnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Variante 5\n",
        "\n",
        "<img src=\"images/reconstruct_svd_gpu_5.png\" alt=\"Report für Variante 5\" width=\"500\"/>\n",
        "\n",
        "<font color=\"blue\">Diese Variante setzt auf das Preloading des nächsten Blocks. Dabei werden weniger `syncthreads` im Code verwendet. Im Report dazu sehen wir, dass so im Vergleich zur Variante 2 die Auslastung des Streaming Multiprocessors und auch der ALU erhöht werden konnte. Es scheinen aber zu wenig Ressourcen (Schedulers) zur Verfügung zu stehen, um genügend Warps ausführen zu können. Viele Warps befinden sich im Zustand, so dass sie eigentlich ausgeführt werden könnten, aber aufgrund der vielen anderen bereiten Warps müssen sie im Schnitt 5 Zyklen warten, bis sie wirklich ausgeführt werden können (*Stall Not Selected*). Auch warten relativ viele Warps auf die Math-Pipeline (mutmasslich die ausgelastete ALU). \n",
        "\n",
        "Da weniger `syncthreads` verwendet wurden, hat sich auch die Wartezeit beim Zustand *Stall Barrier* im Vergleich zu Report 2 reduziert. Die GPU-Komponenten scheinen dadurch also ungleichmässiger ausgelastet zu sein, was sich in einer doppelt so hohen Duration und doppelt so vielen elapsed Cycles ausdrückt.</font>"
      ],
      "metadata": {
        "id": "6elQoqMJS8Te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Variante 6\n",
        "\n",
        "<font color=\"blue\">Um die in Variante 5 identifizierten Probleme zu beheben, wurde eine sechste Variante des Kernels implementiert. Die grösste Optimierung besteht hier darin, dass die Threads per Block von 32 x 32 auf 16 x 16 reduziert wurden, so dass auch die Shared-Memory pro Threadblock verkleinert werden können. Dies in der Hoffnung, dass so mehr Schedulers bereitstehen und mehr Warps gleichzeitig ausgeführt werden können.\n",
        "\n",
        "Zudem wurde eine If-Abfrage eingebaut, so dass nach dem letzten Block keine Schreibzugriffe aufs Shared-Memory mehr durchgeführt werden und es existiert nun nur noch eine `syncthreads`-Anweisung. Diese befindet sich genau vor dem Code-Block bei der das Shared-Memory bereitstehen muss und nicht mehr einige Anweisungen vorher. Damit kann die unnötige `syncthreads`-Anweisung nach dem letzten Block vermieden werden. So kann der Thread früher terminieren und befindet sich zum Schluss nicht noch in einem Wartezustand. Dies wird die Laufzeit nicht wirklich verkürzen, aber diese Warps beeinflussen den Report etwas weniger mit diesem Zustand.</font>"
      ],
      "metadata": {
        "id": "Hwt4-kJxVkxu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### BEGIN SOLUTION\n",
        "@cuda.jit\n",
        "def _reconstruct_svd_gpu_6(u, s, vt, reco):\n",
        "  # each thread calculates the sum of products for a specific index (x, y)\n",
        "\n",
        "  x, y = cuda.grid(2)\n",
        "  local_x = cuda.threadIdx.x\n",
        "  local_y = cuda.threadIdx.y\n",
        "  threads_per_block = 16 # cuda.blockDim.x but must be constant\n",
        "  blocks_per_grid = cuda.gridDim.x\n",
        "\n",
        "  shrd_u = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_s = cuda.shared.array(shape=(threads_per_block,), dtype=float32)\n",
        "  shrd_vt = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_u_pre = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "  shrd_s_pre = cuda.shared.array(shape=(threads_per_block,), dtype=float32)\n",
        "  shrd_vt_pre = cuda.shared.array(shape=(threads_per_block, threads_per_block), dtype=float32)\n",
        "\n",
        "  shrd_u_pre[local_y, local_x] = 0\n",
        "  if y < u.shape[0] and local_x < u.shape[1]:\n",
        "    # copy values in transposed order\n",
        "    shrd_u_pre[local_y, local_x] = u[y, local_x]\n",
        "\n",
        "  if local_y == 0:\n",
        "    shrd_s_pre[local_x] = 0\n",
        "    if local_x < s.shape[0]:\n",
        "      # only first row in block loads shrd_s since it's a one-dimensional array\n",
        "      shrd_s_pre[local_x] = s[local_x]\n",
        "\n",
        "  shrd_vt_pre[local_y, local_x] = 0\n",
        "  if x < vt.shape[1] and local_y < vt.shape[0]:\n",
        "    shrd_vt_pre[local_y, local_x] = vt[local_y, x]\n",
        "\n",
        "  sum_of_products = float32(0.)\n",
        "  for block in range(blocks_per_grid):\n",
        "    # calculate sum of products per block\n",
        "    # the block is only moved to get the other values in the matrices u, s and vt\n",
        "    # the index for which the sum of product is calculated remains the same\n",
        "\n",
        "    # swap shared memory blocks\n",
        "    shrd_u, shrd_u_pre = shrd_u_pre, shrd_u\n",
        "    shrd_s, shrd_s_pre = shrd_s_pre, shrd_s\n",
        "    shrd_vt, shrd_vt_pre = shrd_vt_pre, shrd_vt\n",
        "\n",
        "    next_block = block + 1\n",
        "\n",
        "    # wait until all threads have computed their sum of products before we move\n",
        "    # to the next block as the shared values will be overridden with the next\n",
        "    # iteration\n",
        "    cuda.syncthreads()\n",
        "\n",
        "    if next_block < blocks_per_grid:\n",
        "      # load next block\n",
        "      shrd_u_pre[local_y, local_x] = 0\n",
        "      if y < u.shape[0] and (next_block * threads_per_block + local_x) < u.shape[1]:\n",
        "        # copy values in transposed order\n",
        "        shrd_u_pre[local_y, local_x] = u[y, next_block * threads_per_block + local_x]\n",
        "\n",
        "      if local_y == 0:\n",
        "        shrd_s_pre[local_x] = 0\n",
        "        if (next_block * threads_per_block + local_x) < s.shape[0]:\n",
        "          # only first row in block loads shrd_s since it's a one-dimensional array\n",
        "          shrd_s_pre[local_x] = s[next_block * threads_per_block + local_x]\n",
        "\n",
        "      shrd_vt_pre[local_y, local_x] = 0\n",
        "      if x < vt.shape[1] and (next_block * threads_per_block + local_y) < vt.shape[0]:\n",
        "        shrd_vt_pre[local_y, local_x] = vt[next_block * threads_per_block + local_y, x]\n",
        "\n",
        "    # start calculating the sum of products for index (y, x) and the current block\n",
        "    for i in range(threads_per_block):\n",
        "      # no checking of boundaries necessary since the warp executes the\n",
        "      # statement anyway and threads out of bound would have to wait anyway\n",
        "\n",
        "      # read values from shred_u in transposed order\n",
        "      sum_of_products += shrd_u[local_y, i] * shrd_s[i] * shrd_vt[i, local_x]\n",
        "\n",
        "  if y < reco.shape[0] and x < reco.shape[1]:\n",
        "    reco[y, x] = sum_of_products\n",
        "\n",
        "def reconstruct_svd_gpu_6(u, s, vt, k):\n",
        "  threads_per_block = 16\n",
        "  reco_h = np.zeros((u.shape[0], vt.shape[1]))\n",
        "\n",
        "  reco_d = cuda.to_device(reco_h)\n",
        "  u_d = cuda.to_device(u[:, 0:k])\n",
        "  s_d = cuda.to_device(s[0:k])\n",
        "  vt_d = cuda.to_device(vt[0:k, :])\n",
        "\n",
        "  grid_y_max = max(u.shape[0], k)\n",
        "  grid_x_max = max(k, vt.shape[1])\n",
        "\n",
        "  n_blocks_x = math.ceil(grid_x_max / threads_per_block)\n",
        "  n_blocks_y = math.ceil(grid_y_max / threads_per_block)\n",
        "\n",
        "  _reconstruct_svd_gpu_6[\n",
        "    (n_blocks_x, n_blocks_y), (threads_per_block, threads_per_block)\n",
        "  ](u_d, s_d, vt_d, reco_d)\n",
        "\n",
        "  return reco_d.copy_to_host()\n",
        "\n",
        "### END SOLUTION"
      ],
      "metadata": {
        "id": "CoAMu7gpvzyN"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reco = reconstruct_svd_gpu_6(u, s, vt, u.shape[1])\n",
        "np.testing.assert_array_almost_equal(reco, m, decimal=3)\n"
      ],
      "metadata": {
        "id": "HXqCGw89v9o7"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_runtimes(\n",
        "    ['memory optimised', 'preloading shared memory\\n 32 threads per block', 'preloading shared memory\\n 16 threads per block'],\n",
        "    [reconstruct_svd_gpu_2, reconstruct_svd_gpu_5, reconstruct_svd_gpu_6]\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470
        },
        "id": "S6hvAXImv_Wm",
        "outputId": "37bc923d-139e-4bb4-d9cb-0b41c27558c4"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 864x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"images/reconstruct_svd_gpu_6.png\" alt=\"Report für Variante 6\" width=\"500\"/>\n",
        "\n",
        "<font color=\"blue\">Diese Optimierungen bringen die Laufzeiten in die Grössenordnung der bisher besten Variante. Tendenziell ist diese sogar leicht schneller als Variante 2. Die Duration konnte laut Report von 92.5 ms auf 85 ms verkürzt werden und es sind entsprechend auch weniger Cylces nötig. Streaming Multiprocessor sind durch die Verkleinerung der Warps wieder beide gleich stark ausgelastet und konnte um 10 Prozentpunkte auf 85 % erhöht werden. \n",
        "\n",
        "Aufgrund der kleineren Blöcke haben sich jedoch die Lesezugriffe aufs Global-Memory erhöht. Dies ist auch beim L1- und L2-Cache erkennbar, durch welche die Zugriffe aufs Global-Memory (Device Memory) laufen. Bei der Occupancy sehen wir, dass die Parameter für Block Size und die Grösse des Shared Memory pro Threadblock und der implizite Wert für die Registers pro Thread hinsichtlich der Warp-Auslaustung optimal gewählt wurden.\n",
        "\n",
        "Die längsten Wartezeiten der Warps fallen nun auf die Zustände *Stall LG Throttle* und *Stall Long Scoreboard*, also warten sie auf die Memory-Komponenten um auf das Global- und Shared-Memory zugreifen zu können.</font>"
      ],
      "metadata": {
        "id": "2S81PrDjYq8k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "t0N9LmNh10gI"
      },
      "source": [
        "### 6 Beschleunigte Rekonstruktion mehrerer Bilder\n",
        "#### 6.1 Implementierung\n",
        "Verwende einige der in bisher gelernten Konzepte, um mehrere Bilder gleichzeitig parallel zu rekonstruieren. Weshalb hast du welche Konzepte für deine Implementierung verwenden? Versuche die GPU konstant auszulasten und so auch die verschiedenen Engines der GPU parallel zu brauchen. Diskutiere in ca. 250-300 Wörtern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "d3pBEU7310gI"
      },
      "outputs": [],
      "source": [
        "### BEGIN SOLUTION\n",
        "\n",
        "### END SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "qDbRypeY10gI"
      },
      "source": [
        "<font color='blue'>Antwort hier eingeben</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "1tqRdLjg10gJ"
      },
      "source": [
        "#### 6.2 Analyse\n",
        "Vergleiche den Speedup für deine parallele Implementierung im Vergleich zur seriellen Rekonstruktion einzelner Bilder. Analysiere und diskutiere in diesem Zusammenhang die Gesetze von Amdahl und Gustafson in ca. 300 Wörtern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "G67OGrLS10gJ"
      },
      "source": [
        "<font color='blue'>Antwort hier eingeben</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "VuJpsXdO10gJ"
      },
      "source": [
        "#### 6.3 Komponentendiagramm\n",
        "\n",
        "Erstelle das Komponentendiagramm dieser Mini-Challenge für die Rekunstruktion mehrere Bilder mit einer GPU-Implementierung. Erläutere das Komponentendigramm in 3-4 Sätzen.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "q-3MVyIZ10gJ"
      },
      "source": [
        "<font color='blue'>Antwort hier eingeben inkl. Bild(ern).</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tPwBkA510gK"
      },
      "source": [
        "### 7 Reflexion\n",
        "\n",
        "Reflektiere die folgenden Themen indem du in 3-5 Sätzen begründest und anhand von Beispielen erklärst."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyZYqo6010gK"
      },
      "source": [
        "1: Was sind deiner Meinung nach die 3 wichtigsten Prinzipien bei der Beschleunigung von Code?\n",
        "\n",
        "<font color='blue'>Antwort hier eingeben</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAJLmN7F10gK"
      },
      "source": [
        "2: Welche Rechenarchitekturen der Flynnschen Taxonomie wurden in dieser Mini-Challenge wie verwendet?\n",
        "\n",
        "<font color='blue'>Antwort hier eingeben</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8LPwpR710gK"
      },
      "source": [
        "3: Haben wir es in dieser Mini-Challenge hauptsächlich mit CPU- oder IO-Bound Problemen zu tun? Nenne Beispiele.\n",
        "\n",
        "<font color='blue'>Da sich die Bilder bereits im Arbeitsspeicher befinden, haben wir es vor allem mit CPU-Problemen zu tun. Die Berechnung hat eine Komplexität von $O(n^3)$ und ist somit sehr rechenintensiv.</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sR_rs5B510gK"
      },
      "source": [
        "4: Wie könnte diese Anwendung in einem Producer-Consumer Design konzipiert werden?\n",
        "\n",
        "<font color='blue'>Antwort hier eingeben</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQtna3S210gL"
      },
      "source": [
        "5: Was sind die wichtigsten Grundlagen, um mehr Performance auf der GPU in dieser Mini-Challenge zu erreichen?\n",
        "\n",
        "<font color='blue'>Antwort hier eingeben</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "uwLJyI_810gL"
      },
      "source": [
        "6: Reflektiere die Mini-Challenge in ca. 300-500 Zeichen. Was ist gut gelaufen? Wo gab es Probleme? Wo hast du mehr Zeit als geplant gebraucht? Was hast du dabei gelernt? Was hat dich überrascht? Was hättest du zusätzlich lernen wollen? Würdest du gewisse Fragestellungen anders formulieren? Wenn ja, wie?\n",
        "\n",
        "<font color='blue'>Antwort hier eingeben</font>"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}